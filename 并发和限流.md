## 实现并发的几种方式

java实现并发的方式大致如下：

1. new Thread：每次的任务都开启新的线程执行，优点是线程数量可以动态改变，与之而来的也是线程数量以及出现的问题不可控，开销大。
   
   1. 继承Thread类
   2. 实现Runnable接口
   3. 实现Callable接口

2. 线程池：线程的管理效率高，线程可回收利用，减少资源消耗。但线程最大数量不能动态改变，如果一开始线程池参数设置不合理，执行任务的效率可能就有点差强人意了。
   
   ```java
   public static void main(String[] args) {
       ExecutorService executorService = Executors.newFixedThreadPool(5);
       for(int i=0; i<5; i++){
           int finalI = i;
           executorService.execute(()-> {
               System.out.println(finalI);
           });
       }
   }
   ```

3. jdk1.8的parallelStream：parallelStream的底层是ForkJoin框架，它同ThreadPoolExecutor一样，也实现了Executor和ExecutorService接口。它使用了一个无限队列来保存需要执行的任务，而线程的数量则是通过构造函数传入，如果没有向构造函数中传入希望的线程数量，那么当前计算机可用的CPU数量会被设置为线程数量作为默认值。但不建议使用parallelStream去处理IO密集的任务。
   
   ```java
   list.parallelStream().forEach(p->{
       //将所有请求在打印之前进行阻塞，方便观察
       countDownLatch.countDown();
       try {
           System.out.println("线程执行了---p");
       } catch (InterruptedException e) {
           e.printStackTrace();
       }
       System.out.println(p);
   });
   ```

## 并发控制的几种方式

- 同步代码块synchronized：通过同步代码块，来确保同一时刻只会有一个线程执行对应的业务逻辑。
- 锁Lock：jdk本身提供了不少的锁，为了实现单实例的并发控制，我们需要选择写锁；如果支持多读，单实例写，则可以考虑读写锁
- CAS自旋：比如AtomicXXX原子类中的很多实现，就是借助unsafe的CAS来实现的
- 信号量Semaphore：通过设置信号量，来控制并发数
- 计数器CountDownLatch：应用场景更偏向于多线程的协同，比如多个线程执行完毕之后，再处理某些事情；
- 栅栏 CyclicBarrier：与上面的CountDownLatch相似，区别在于正向计数+1, 只有达到条件才放行; 且支持通过调用reset()重置计数，而CountDownLatch则不行

## 限流的几种方式

在开发高并发系统时有三把利器用来保护系统：缓存、降级和限流

- 缓存：缓存的目的是提升系统访问速度和增大系统处理容量
- 降级：降级是当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行
- 限流：限流的目的是通过对并发访问/请求进行限速，或者对一个时间窗口内的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务、排队或等待、降级等处理

### 限流分类

限流的实现方案有很多种，大致分类如下所示：

1. **合法性验证限流**：比如验证码、IP 黑名单等，这些手段可以有效的防止恶意攻击和爬虫采集；
2. **容器限流**：比如 Tomcat、Nginx等限流手段，其中 Tomcat 可以设置最大线程数（maxThreads），当并发超过最大线程数会排队等待执行；而 Nginx 提供了两种限流手段：一是控制速率，二是控制并发连接数；
3. **服务端限流**：比如我们在服务器端通过限流算法实现限流。

## **容器限流**

### Tomcat 限流

Tomcat 8.5 版本的最大线程数在 conf/server.xml 配置中，如下所示：

```xml
<Connector port="8080" 
           protocol="HTTP/1.1"
           connectionTimeout="20000"
           maxThreads="150"
           redirectPort="8443" />
```

其中 `maxThreads` 就是 Tomcat 的最大线程数，当请求的并发大于此值（maxThreads）时，请求就会排队执行，这样就完成了限流的目的。

>   需要注意一下，操作系统对于进程中的线程数有一定的限制，Windows 每个进程中的线程数不允许超过 2000，Linux 每个进程中的线程数不允许超过 1000。

### Nginx 限流

Nginx 提供了两种限流手段：一是控制速率，二是控制并发连接数。

#### 控制速率

我们需要使用 `limit_req_zone` 用来限制单位时间内的请求数，即速率限制，示例配置如下：

```bash
limit_req_zone $binary_remote_addr zone=mylimit:10m rate=2r/s;
server { 
    location / { 
        limit_req zone=mylimit;
    }
}
```

以上配置表示，限制每个 IP 访问的速度为 2r/s，因为 Nginx 的限流统计是基于毫秒的，我们设置的速度是 2r/s，转换一下就是 500ms 内单个 IP 只允许通过 1 个请求，从 501ms 开始才允许通过第 2 个请求。

真实情况下我们应该控制一个 IP 单位总时间内的总访问次数，而不是像上面那么精确到毫秒，我们可以使用 **burst 关键字**开启此设置，示例配置如下：

```bash
limit_req_zone $binary_remote_addr zone=mylimit:10m rate=2r/s;
server { 
    location / { 
        limit_req zone=mylimit burst=4;
    }
}
```

开启之后如果500ms内可以接收一个请求并立即处理，此外还可以接收4个请求，放在 burst 队列里排队执行。

#### 控制并发数

利用 `limit_conn_zone` 和 `limit_conn` 两个指令即可控制并发数，示例配置如下：

```bash
limit_conn_zone $binary_remote_addr zone=perip:10m;
limit_conn_zone $server_name zone=perserver:10m;
server {
    ...
    limit_conn perip 10;
    limit_conn perserver 100;
}
```

其中 limit_conn perip 10 表示限制单个 IP 同时最多能持有 10 个连接；limit_conn perserver 100 表示 server 同时能处理并发连接的总数为 100 个。

>   只有当 request header 被后端处理后，这个连接才进行计数

## 服务端限流

服务端限流需要配合限流的算法来执行，而算法相当于执行限流的“大脑”，用于指导限制方案的实现。

有人看到「算法」两个字可能就晕了，觉得很深奥，其实并不是。算法就相当于操作某个事务的具体实现步骤汇总，其实并不难懂，不要被它的表象给吓到哦~

**限流的常见算法**有以下三种：

1. 时间窗口算法
2. 漏桶算法
3. 令牌算法

### 时间窗口算法

将固定窗口中分割出多个小时间窗口，分别在每个小的时间窗口中记录访问次数，当计数器达到设定限制后，该窗口时间内的之后的请求都被丢弃处理。该窗口时间结束后，计数器清零，从新开始计数。

**优点**：实现简单，并且**内存占用小**，我们只需要存储时间窗口中的计数即可;

**缺点**：流量曲线可能不够平滑，有“突刺现象”（2N），具体表现如下：

- 一段时间内（不超过时间窗口）系统服务不可用。
  例如：窗口大小为1s，限流大小为100，然后恰好在某个窗口的第1ms来了100个请求，然后第2ms-999ms的请求就都会被拒绝，这段时间用户会感觉系统服务不可用。

- 瞬时流量的临界问题：窗口切换时可能会产生两倍于阈值流量的请求
  
  例如：窗口大小为1s，限流大小为100，然后恰好在某个窗口的第999ms来了100个请求，窗口前期没有请求，所以这100个请求都会通过。再恰好，下一个窗口的第1ms有来了100个请求，也全部通过了，那也就是在2ms之内通过了200个请求，而我们设定的阈值是100，通过的请求达到了阈值的两倍。

​         

### 漏桶算法

漏桶算法类似于生活中的漏斗，无论上面的水流倒入漏斗有多大，也就是无论请求有多少，它都是以均匀的速度慢慢流出的。当上面的水流速度大于下面的流出速度时，漏斗会慢慢变满，当漏斗满了之后就会丢弃新来的请求;当上面的水流速度小于下面流出的速度的话，漏斗永远不会被装满，并且可以一直流出。

漏桶算法的实现步骤：先声明一个队列用来保存请求，这个队列相当于漏斗，当队列容量满了之后就放弃新来的请求，然后重新声明一个线程定期从任务队列中获取一个或多个任务进行执行，这样就实现了漏桶算法。

### 令牌算法

在令牌算法中有一个程序以某种恒定的速度生成令牌，并存入令牌桶中，而每个请求需要先获取令牌才能执行，如果没有获取到令牌的请求可以选择等待或者放弃执行。如果令牌桶满了，则该令牌直接丢弃。

Google 开源的 guava 包下的RateLimiter实现令牌桶算法。

```java
// 创建qps限流器
RateLimiter rateLimiter = RateLimiter.create(5);
list.parallelStream()
    .forEach(e -> {
        if(rateLimiter.tryAcquire(3, TimeUnit.SECONDS)){
            // do something
        } else {
            log.error("并发执行失败，原因是等待超时！");
        }
    });
```
