# 什么是分布式事务？

在分布式系统环境下由不同的服务之间通过网络远程协作完成的事务称之为分布式事务，例如用户注册送积分事务、创建订单减库存事务，银行转账事务等都是分布式事务。

分布式事务产生的场景：

1. 多个服务+单个数据库实例：

2. 单个服务+多个数据库实例：用户信息和订单信息分别在两个MySQL实例存储，用户管理系统删除用户信息，需要分别删除用户信息及用户的订单信息，由于数据分布在不同的数据实例，需要通过不同的数据库链接去操作数据，此时产生分布式事务。

3. 多个微服务+多个数据实例：

# 分布式事务基础理论

通过理论知识可以指导我们确定分布式事务控制的目标，从而帮助我们理解每个解决方案。

 

## CAP理论

CAP是 Consistency、Availability、Partition tolerance三个词语的缩写，分别表示一致性、可用性、分区容忍性。

- Consistency一致性是指写操作后的读操作可以读取到最新的数据状态，当数据分布在多个节点上，从任意结点读取到的数据都是最新的状态。实现一致性一般需要进行节点之间的同步操作，比较消耗性能。

- Availability可用性是指任何事务操作都可以得到响应结果，且不会出现响应超时或响应错误。一般可用性允许节点直接存在一定数据的不一致，来保证节点的可用性。

- Partition tolerance分区容忍性：分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供服务。

分区容忍性是分布式系统具备的基本能力。在所有分布式事务场景中不会同时具备CAP三个特性，因为在具备了P的前提下C和A是不能共存的。

所以CAP有以下组合，可以根据特定场景选择合适的分布式目标：

- CA：放弃分区容错性P，加强一致性和可用性，其实就是传统的单机数据库的选择。

- AP：放弃一致性C，一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。

- CP：放弃可用性A，由于分区之间会导致同步时间无限延长(等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据一致了之后再让用户访问系统。

## BASE理论

BASE是**B**asically **A**vailable（基本可用）、**S**oft state（软状态）和**E**ventually consistent（最终一致性）的缩写，基于CAP定理逐步演化而来。

BASE理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到**最终一致性**。

- 基本可用：分布式系统在出现不可预知故障的时候，允许损失部分可用性，从而保证核心功能的可用性。比如时间上的损失，功能上的损失。

- 软状态：允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。

- 最终一致性：最终一致性强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。

# 分布式事务的解决方案

![](https://pic1.zhimg.com/v2-84c272f5b9370ac87d9c02a6ec0b51d8_r.jpg)

## 2PC(两阶段提交)

2PC即两阶段提交协议，是将整个事务流程分为两个阶段，准备阶段（Prepare phase）、提交阶段（commit phase）。

两阶段提交协议的角色有：事务管理器（决策者）和资源管理器（参与者）

事务管理器负责向资源管理器发送指令，收集参与者的反馈，做出提交或者回滚决策
资源管理器接收事务管理器的指令执行事务操作，向事务管理器反馈操作结果，并继续执行事务管理器发送的最终指令。

实现2PC的方案有两个：

- XA方案

- Seata方案

缺点：

1. 性能问题：从事务开始到事务提交或回滚，这期间所有参与者的资源一致处于锁定状态。

2. 数据不一致：极端情况下不管是由于协调者故障，还是网络分区都会有导致数据不一致的风险。

3. 阻塞问题：如果协调者在第一阶段就挂掉了，则所有参与者都会阻塞。

### XA方案

基于数据库的XA规范来实现2PC称为XA方案。XA 规范 描述了全局的事务管理器与局部的资源管理器之间的接口。

为了统一标准，减少行业内不必要的对接成本，国际开放标准组织定义了标准化的分布式事务处理模型**DTP**（Distributed Transaction Processing Reference Model）。

DTP模型定义如下角色：

- AP(Application Program)：即应用程序，可以理解为使用DTP分布式事务的程序。

- RM(Resource Manager)：即资源管理器，可以理解为事务的参与者，一般情况下是指一个数据库实例，通过资源管理器对该数据库进行控制，资源管理器控制着分支事务。

- TM(Transaction Manager)：事务管理器，负责协调和管理事务，事务管理器控制着全局事务，管理事务生命周期，并协调各个RM。

执行流程如下：

1. 在准备阶段RM执行实际的业务操作，但不提交事务，资源锁定；

2. 在提交阶段TM会接受RM在准备阶段的执行回复，只要有任一个RM执行失败，TM会通知所有RM执行回滚操作，否则，TM将会通知所有RM提交该事务。提交阶段结束资源锁释放。

XA方案的问题：

1、需要本地数据库支持XA协议。

2、同步等待，资源锁需要等到两个阶段结束才释放，性能较差。

### Seate AT方案

TC、TM、RM：TC作为事务协调器决定提交还是回滚，以及生成全局事务id和分支事务id，然后让TM去通知RM进行提交或回滚。

Seata实现2PC与传统2PC的差别：

架构层次方面，传统2PC方案的 RM 实际上是在数据库层，RM 本质上就是数据库自身，通过 XA 协议实现，而Seata的 RM 是以jar包的形式作为中间件层部署在应用程序这一侧的。

两阶段提交方面，传统2PC无论第二阶段的决议是commit还是rollback，事务性资源的锁都要保持到Phase2完成才释放。而Seata的做法是在Phase1 就将本地事务提交，这样就可以省去Phase2持锁的时间，整体提高效率。

seata at 模式默认的隔离级别为读未提交（因为已经提交的sql有可能会回滚）。如果要实现读已提交，select语句需要更改为 SELECT FOR UPDATE 语句。

SELECT FOR UPDATE 语句的执行会向TC申请这些数据的行锁，如果这些数据的锁被其他事务持有，则会等待锁的释放（即读取的相关数据是已提交的）。这个过程中，查询是被 block 住的。在TC中会对写操作以及select for update的sql进行拦截并记录，从而形成全局锁，其他分布式事务使用数据前需要先获取锁，避免当前分布式事务影响其他事务。

缺点：

- 需要创建undolog表进行回滚。

- 由于持有全局锁，在高并发下会有性能问题。

- AT 模式下，默认的数据库隔离级别是: 读取未提交。这一点需要注意，在涉及到强一致性的读，需要加上全局锁，否则会出现脏读。

## TCC补偿事务

TCC（Try-Confirm-Cancel）分布式事务模型相对于 XA 等传统模型，其特征在于它不依赖资源管理器（RM）对分布式事务的支持，而是通过对业务逻辑的分解来实现分布式事务。

首先TCC在事务执行前，通过预留资源的方式把需要的资源事先隔离出来，这样就保证这部分的资源不被其它事务再次使用，然后为操作失败的情况制定了一个补偿操作来进行业务回滚，所以TCC的模式里是允许业务失败的。

TCC分为Try、Confirm、Cancel三个方法， Try阶段主要是进行资源预留，如果Try阶段执行成功，那么本次事务所需要的资源都已经获得，这样在事务提交的时候肯定能提交成功。如果Try阶段失败，那么此时就会调用Cancel操作进行资源释放。

- Try阶段：该阶段主要做业务可行性检查并预留事务所需要的资源，保证预留出来的资源与其它的事务隔离开来。

- Confirm：当所有事务参与者在Try阶段都执行成功后，进入Confirm阶段。此阶段将直接使用Try阶段预留资源进行业务操作。

- cancel: 任意一个参与者在当Try阶段执行失败，则整体进入Cancel阶段，此阶段将释放Try阶段预留的资源。

TCC需要注意三种异常处理分别是**空回滚、幂等、悬挂**:

**幂等**：为了保证TCC二阶段提交重试机制不会引发数据不一致，要求 TCC 的二阶段 Try、Confirm 和 Cancel 接口保证幂等，这样不会重复使用或者释放资源。如果幂等控制没有做好，很有可能导致数据不一致等严重问题。

- 解决思路是每个分支都使用**事务状态表**（包含事务 ID ，分支事务 ID，事务状态），每次执行前都查询事务状态，事务状态为Tried的时候才执行Confirm / Cancel。

**空回滚**：**在没有调用Try 方法的情况下，调用了 Cancel 方法**，Cancel 方法需要识别出这是一个空回滚，然后直接返回成功。出现原因是当一个分支事务所在服务宕机或网络异常，分支事务调用记录为失败，这个时候其实是没有执行Try阶段，当故障恢复后，分布式事务进行回滚则会调用二阶段的Cancel方法，从而形成空回滚。

- 解决思路是每个分支都使用“**try日志表**”（包含事务 ID 和分支事务 ID），执行 Try 方法里会插入一条记录，表示一阶段执行了。如果执行Cancel 接口则先读取该记录，如果该记录存在，则正常回滚；如果该记录不存在，则是空回滚。

**悬挂**：资源悬挂问题发生在**cancel比try先执行**， 如果发送Try请求时发生网络延时，那么请求方会认为业务失败，此时会进入Cancel阶段，当所有的Cancel都执行完之后，整个事务结束。但是之前发生网络延时的请求最终还是发送到了对应的服务，此时会执行Try操作，但是事务已经结束了，没有任何后续流程来提交或者释放Try阶段预留的资源。

- 解决思路是使用“**cancel日志**”表中（包含事务 ID 和分支事务 ID）记录Cancel行为，在执行try之前判断是否执行了cancel，如果有则不执行Try。

优点：

- 性能：不会锁整个资源，只会锁对应业务数据的资源，粒度更小。

- 可靠性：避免2pc协调者单点故障问题。

- 数据一致性：通过业务行为保证数据的最终一致性。

缺点：业务侵入性强，与业务强耦合。

## 可靠消息最终一致性

可靠消息最终一致性方案是指当事务发起方执行完成本地事务后并发出一条消息，事务参与方(消息消费者)一定能够接收消息并处理事务成功，此方案强调的是只要消息发给事务参与方则最终事务肯定会达到一致。

在可靠消息最终一致性方案中需要保证消息一定会发送给消费方，所以需要保证发送方消息发送的可靠性，mq消息的可靠性，以及消费者消费的可靠性和幂等性。

- 发送方消息发送的可靠性：本地事务表+轮询保证消息保证消息一定发送出去，轮询时使用mq的手动ack与消息回调。

- mq消息的可靠性：使用消息与队列的持久化保证消息的可靠性。

- 消费者消费的可靠性：使用mq的手动确认机制。

- 消费者消费的幂等性：使用消费表记录消息不会重复被消费。

## 最大努力通知

事务发起方执行完成本地事务后发出一条消息，消息不保证一定可以到达事务参与方，但发起方会提供一个接口给参与方查询消息的内容，如果参与方在规定时间内收不到消息，可以自行查询消息接口。

最大努力通知解决方案有两种：

1. 发起方跟参与方监听同一个消息主题，参与方如果规定时间内没有消费到消息则收到回调发起方接口。适合公司内部使用。

2. 发起方单独使用一个消息通知服务，发起方与消息通知服务监听同一个消息主题，当消息通知服务收到消息之后调用参与方的接口通知其调用发起方接口查询消息内容。适合公司外部使用。

小结：

2PC场景比较适用于内部使用，更强调一致性。

TCC更多的是在业务层进行保证最终一致性。

可靠消息最终一致性跟最大努力通知更强调可用性。可靠消息最终一致性比较适用于内部使用。

# 分布式id

在业务开发中，会存在大量的场景都需要唯一 ID 来进行标识。比如，用户需要唯一身份标识；商品需要唯一标识；消息需要唯一标识。尤其是在分布式场景下，业务会更加依赖唯一 ID。

分布式唯一 ID 特性：

- 全局唯一：必须保证生成的 ID 是全局性唯一的，这是分布式 ID 的基本要求；

- 有序性：生成的 ID 需要按照某种规则有序，便于数据库的写入和排序操作；

- 可用性：需要保证高并发下的id生成服务的可用性，避免影响主业务。

- 安全性：不暴露系统和业务的信息。

## 常用分布式唯一 ID 生成方案

常用的分布式唯一id生成方案有如下几种：

1. uuid

2. 数据库自增id

3. redis生成id

4. snowflake雪花算法

### UUID

UUID算法的目的是生成某种形式的全局唯一 ID 来标识系统中的任一元素，尤其是在分布式环境下，UUID 可以不依赖中心认证即可自动生成全局唯一 ID。

UUID 的标准形式为 **32 个十六进制数组成的字符串 + 4个分割符组成**，例如：467e8542-2275-4163-95d6-7adc205580a9。

UUID 的优势是性能非常高，由于是本地生成，没有网络消耗。而其也存在一些缺陷，包括UUID 太长不易于存储（通常以 36 长度的字符串表示）；UUID 作为 DB 主键时在插入跟查询效率变慢（因为uuid无序所以插入的时候需要维护索引，且存储较大导致分支变少，查询速度也会降低）。

基于使用场景的不同，会存在以下几个不同版本的 UUID 以供使用，如下所示：

- 基于时间的 UUID：主要依赖当前的时间戳和机器 mac 地址。优势是能基本保证全球唯一性，缺点是由于使用了 mac 地址，会暴露 mac 地址和生成时间；

- 基于随机数的 UUID：基于随机数或伪随机数生成。优势是实现简单，缺点是重复几率可计算；

- 基于名字空间的 UUID（MD5 版）：基于指定的名字空间/名字生成 MD5 散列值得到。优势是不同名字空间/名字下的 UUID 是唯一的，缺点是 MD5 碰撞问题，只用于向后兼容；

### 数据库自增id

数据库自增 ID 是最常见的一种生成 ID 方式。利用数据库本身来进行设置，在全数据库内保持唯一。优势是使用简单，满足基本业务需求，天然有序；缺点是强依赖 DB，存在单点故障问题、不同机器下id重复问题。

针对上面介绍的数据库自增 ID 的缺陷，会存在以下两种优化方案：

- 数据库水平拆分：设置不同的初始值和相同的步长。这样可以有效的生成集群中的唯一 ID，也大大降低 ID 生成数据库操作的负载。

- 批量生成一批 ID：服务端每次从数据库获取一批id，在需要分配id时在服务端分配，这样可以将数据库的压力减小到先前的 N 分之一，且数据库故障后仍可继续使用一段时间。此种方法详见下面的**数据库号段模式**介绍。

### Redis 生成 ID

当使用数据库来生成 ID 性能不够的时候，可以尝试使用 Redis 来生成 ID。主要使用 Redis 的原子操作 INCR 和 INCRBY 来实现。优势是不依赖于数据库，使用灵活，性能也优于数据库；而缺点则是可能要引入新的组件 Redis，如果 Redis 出现单点故障问题，则会影响序号服务的可用性。

### Snowflake 雪花算法

snowflake(雪花算法)是一个开源的分布式 ID 生成算法，结果是一个 long 型的 ID。snowflake 算法将**64bit**划分为多段，分开来标识时间、机器等信息，具体组成结构如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavZUO55FfNL5S6Z9Gr39mNOyics0jZJautPdUIIKtrB992cXypYvGsrfm6tEdqfyxOOVRf3yyDrkRA/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

snowflake 算法的核心思想是使用 41bit 作为毫秒数，10bit 作为机房/机器的 ID,12bit 作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是 0。

snowflake 算法可以根据自身业务的需求进行一定的调整：

- 针对10位的机器位和12位的流水位：估算未来的数据中心/机器个数 以及一毫秒内的并发数 来调整在 各自所需要的 bit 数。

- 41bit 作为毫秒数：41位的毫秒数最多可表示69年，默认这69年是针对1970年1月1日的。可自行让这41位的毫秒数的相对时间是我们自定义的时间，即拿到当前时间的毫秒数减去自定义时间的毫秒数即可。

snowflake 算法的优势：

1. 稳定性高，不依赖于数据库等第三方系统；

2. 使用灵活方便，可以根据业务需求的特性来调整算法中的 bit 位；

3. 单机上 ID 单调自增，整个 ID 是趋势递增的(毫秒数在高位，自增序列在低位)。

缺点：

1. 强依赖机器时钟，如果机器上**时钟回拨**，会导致发号重复或者服务处于不可用状态；解决方式有两个：一是等待时钟回拨完成，而是直接报错让业务层处理。

2. ID 可能不是全局递增，虽然 ID 在单机上是递增的，但是由于涉及到分布式环境下的每个机器节点上的时钟，可能会出现不是全局递增的场景。

### 数据库号段模式

**号段模式**是当下分布式 ID 生成器的主流实现方式之一，号段模式可以理解成从数据库批量获取 ID，然后将 ID 缓存在本地，以此来提高业务获取 ID 的效率。例如，每次从数据库获取 ID 时，获取一个号段，如(1,1000]，这个范围表示 1000 个 ID，业务应用在请求获取 ID 时，只需要在本地从 1 开始自增并返回，而不用每次去请求数据库，一直到本地自增到某个阈值时，才去数据库重新获取新的号段，后续流程循环往复。

#### 美团 Leaf-segment 方案

美团之前使用的是号段模式来生成分布式id。

Leaf-server 采用了预分发的方式生成 ID，即可以在 DB 之上挂 N 个 Server，每个 Server 启动时，都会去 DB 拿固定长度的 ID List。这样就做到了完全基于分布式的架构，同时因为 ID 是由内存分发，所以也可以做到很高效。接下来是数据持久化问题，Server 每次去 DB 拿固定长度的 ID List，然后把最大的 ID 持久化下来，也就是并非每个 ID 都做持久化，仅仅持久化一批 ID 中最大的那一个。其流程如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavZUO55FfNL5S6Z9Gr39mNOTGce6icnu6bagfwaFGcdVmKV7OtletepH4K9n0tibeHicalcEbvytdAAQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

Leaf-server 中缓存的号段耗尽之后再去数据库获取新的号段，可以大大地减轻数据库的压力。对 max_id 字段做一次 update 操作，update max_id = max_id + step，update 成功则说明新号段获取成功，新的号段范围为(max_id, max_id + step]。

**为了解决从数据库获取新的号段而阻塞业务获取 ID 的流程的问题，Leaf-server 中采用了异步更新的策略，同时通过双 buffer 的方式**，如下图所示。通过这样一种机制可以保证无论何时 DB 出现问题，都能有一个 buffer 的号段可以正常对外提供服务，只有 DB 在一个 buffer 的下发周期内恢复，都不会影响这个 Leaf 集群的可用性。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavZUO55FfNL5S6Z9Gr39mNOuAZ41Dsg1kMNBT0vPURYxibop16VmhLTicqghdaHKZzibYYhOhW9y9C4Q/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

这种方案依然存在一些问题，它仍然依赖 DB的稳定性，需要采用主从备份的方式提高 DB的可用性，还有 Leaf-segment方案生成的ID是趋势递增的，这样ID号是可被计算的，例如订单ID生成场景，通过订单id号相减就能大致计算出公司一天的订单量，这个是不能忍受的。

#### 滴滴 Tinyid 方案

Tinyid 方案是在 Leaf-segment 的算法基础上升级而来，不仅支持了数据库多主节点模式，还提供了 tinyid-client 客户端的接入方式，使用起来更加方便。

Tinyid 会将可用号段加载到内存中，并在内存中生成 ID，可用号段在首次获取 ID 时加载，如当前号段使用达到一定比例时，系统会异步的去加载下一个可用号段，以此保证内存中始终有可用号段，以便在发号服务宕机后一段时间内还有可用 ID。实现原理如下所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavZUO55FfNL5S6Z9Gr39mNOFaQAu2icejH0iah3aFRpzpUBtkfLf6hDrNsZeRKuz5oLlnFAKQiaVfiaXQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

Tinyid-server只负责id列表的生成，而Tinyid-client则负责从Tinyid-server获取id列表，用户程序则直接使用Tinyid-client获取id即可。

### 雪花模式

由于雪花算法强依赖于机器时间，如果机器上的时钟发生回拨，则可能引起生成的 id 冲突的问题。解决该问题的方案如下所示：

- **直接报错**：当遇到时钟回拨问题时直接报错，交给上层业务来处理；

- **超时等待**：如果回拨时间较短，在耗时要求范围内，比如 5ms，等待回拨时长后在生成 id 返回给业务侧；否则超过这个时间则报错，让业务侧处理。

- **匀出回拨位**：如果回拨时间很长，无法等待，可以匀出少量位作为回拨位，一旦时间回拨，将回拨位加 1，可得到不一样的 ID，2 位回拨可允许标记三次时钟较长时间的回拨，基本够使用。如果超过回拨次数，可以再选择报错或抛出异常。

#### 美团 Leaf-snowflake 方案

Leaf-snowflake 方案沿用 snowflake 方案的 bit 位设计，即”1+41+10+12“的方式组装 ID 号【正数位（占 1 比特）+ 时间戳（占 41 比特）+ 机器 ID（占 5 比特）+ 机房 ID（占 5 比特）+ 自增值（占 12 比特）】。

Leaf-snowflake是解决时钟回拨问题？

1. 服务启动时：检查是否有写过zookeeper leaf_forever 节点；有的话判断机器时间是否小于写节点的时间，是的话认为回拨了启动报警。否则取其他服务创建节点时间的平均值判断是否小于当前时间，是的话认为回拨了启动报警。

2. 服务运行时：会检查时钟回拨时间是否小于 5ms，若时钟回拨时间小于等于 5ms，等待时钟回拨时间后，重新产生新的 ID；若时钟回拨时间大于 5ms，直接抛异常给到业务侧。
